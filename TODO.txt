Use sincos
Integrate newer tinyobj
Fix env map black bug

* SSAO
** Tweak filter sizes+offsets to perfectly match the seed distribution/period
*** Seed = x * samples + y * bandwidth ish
*** Or seed(xy % (2 * bandwidth + 1)
*** If 256 samples are enough, then a 4x4/8x8 filter with 8 samples should do.
** Scalable ambient obscurance.
*** Perhaps sort the samples from inside to out, to make the lookups go from high res to low res texture. Profile!
** Apply power function to uv_samples and reverse sampling so f(x) / PDF(x) = 1'ish
** Bent normal
*** For diffuse env map lookup.
** GTSO / Bent cone
*** Union with light cone / distribution
*** Perhaps only output the bent cone and drop AO values.
** TODOs
** Remove pixelated noise on the rings in the test scene.
*** Depth fighting. Depth bias when rendering. Only if it can be reproduced on other machines though.
*** Drop depth writes during opaque pass?
** HBAO/GTAO??

* Blog post?
** Sampling 
*** Sample02 + reverse_morton_code hash
*** McGuires samples
*** Handle samples outside the buffer
**** Guard band
**** Discard samples
**** Wrap around samples.
**** Clamp screen space radius.
** Filtering
*** Create a ground truth for comparison
** Optimizations
*** Precompute samples (or inline in shaders)
*** Octahedral normal encoding (faster g-buffer + sampling)
*** SAO depth buffer + sorted samples.
*** Temporal reprojection
** Sources:
*** McGuire 11 and 12, G3D
*** Unreal, bilateral
*** GTAO/GTSO

* IBL Convolution should use (RNG::sample02(i) + rng_scramble) % 1.0 instead of RNG::sample02(i, rng_scramble) as that should give fewer blob patterns and more high frequent noise.
** Or owen scrambling? Cranley patterson?


BACKLOG:
* DX11OptiXAdaptor: Render directly to DX11 texture. Wrap a mapped DX11 texture in a CUDA surface and copy the accumulation buffer to it (in a kernel?)
* Screenshot
** Request backbuffer through the composer. (when the image is converged)
*** Request to composer or through the camera.
*** Expose certain backbuffer state pr camera/viewport in a renderer interface. Fx iteration_count, is_continuous_renderer and so on. Members that can be used to evaluate the 'doneness' of a backbuffer.
**** Investigate and potentially use C++ async concepts for this.
*** Both HDR and LDR buffers. HDR buffers should not be tone mapped, color corrected or gamma corrected.
** Render to offscreen buffer stored on the camera.
*** Let it iterate for multiple frames to converge.
** Render to window if a window is specified. Can a window and framebuffer have separate viewports? And then what? Render with the highest resolution and downscale?
* Faster convergence
** Reverse halton sequence
** Faure-permuted Halton sequence sample, http://gruenschloss.org/
* OptiX (filtering) backends
** Try OptiX 5 AI denoiser
** Filter neighbouring light samples before reconstruction - Screen space photon mapping approach with MIS weights / Path filtering.
*** Output buffers: Material ID, texcoord, position, normal, ray dir, weight, light sample, indirect sample. Define for compressing directions and use half/byte for some values.
*** Output screen space sample based on path/BSDF PDF.
**** Perhaps select a vertex if the BSDF PDF is less than 1 / PI or maybe even a bit more. Then vertices are selected roughly proportional to how 'diffuse' they are.
**** Trace ray footprint by assuming the ray is a cone. Then use Distributions::Cone::CosTheta(float angle) to compute the angle of each new cone after an intersection and use that for the density estimation kernel bandwidth.
** Pass BSDF and light sample to the path tracing RGP and let it handle accumulated radiance.
* GLFW 3.3 - Handle windows high DPI scaling.
* Material improvements
** Metal specular tint should go to white at grazing angles.
** Importance sample improvement?
*** The black rubber ball in TestScene produces a lot of nearly black samples. Must be from GGX?
** Rho approximation.
*** GGX contribution seems too strong. Materials are too bright.
*** Perhaps computing the relation between GGX tint and rho would help figure out how much light scatters to the diffuse layer and how much the specular metal layers will reflect?
** Add coat.
*** http://jcgt.org/published/0003/04/03/paper-lowres.pdf and https://www.youtube.com/watch?v=4nKb9hRYbPA
*** Or just add a GGX layer on top with an iridescence parameter (red and blue fresnel offset relative to green.)
* Normals revisited
** Offset slightly along the geometric normal as well? To avoid self shadowing and hard edges on tesselated balls.
*** Or perhaps offset along the incoming direction? That one should obviously not intersect anything else.
** Bump mapping
*** Filter bumpmap mipmaps using Toksvig05, SGGX or AGAA NDF construction.
*** Compute AO maps based on normal maps.
** 'Fix' the normals at edges. Let the shading normal lerp towards the geometric normal at grazing angles. (Offset normal by -view_dir until the dot product is 0.)
* BTDF
** Transmission factor.
*** Use IOR computed from specularity.
* Multiple scenes.
** Only nodes in a certain scene should be rendered.
** Should a scene node know if it is in a scene and in which? Store the root scene ID pr node?'
** When loading models/scenes, store them in scene 0, i.e. the invalid one.
* 3DS, STL and PLY loader.
* Serialize scene and models, perhaps just build GLFT importer and exporter at first. https://github.com/KhronosGroup/glTF
** https://github.com/syoyo/tinygltf

libs
* RtAudio wrapper - https://github.com/thestk/rtaudio
* LuaJit - OpenSource (Fallback to interpretted Lua if the target architecture isn't supported.)
* Emscripten